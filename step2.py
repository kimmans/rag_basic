import logging
import time
from pathlib import Path
import json
import base64
from PIL import Image
import voyageai
from qdrant_client import QdrantClient
from qdrant_client.http import models
import uuid
import re
from dotenv import load_dotenv
import os

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(level=logging.INFO)
    return logging.getLogger(__name__)

def preprocess_korean_text(text):
    """í•œê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text.strip())
    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)
    return text

def chunk_text(text, max_length=1000):
    """í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ì²­í‚¹"""
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        if len(current_chunk) + len(sentence) < max_length:
            current_chunk += sentence + ". "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + ". "
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks if chunks else [text]

def load_processed_data():
    """step1ì—ì„œ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¡œë“œ"""
    
    parsed_dir = Path("data/parsed")
    
    if not parsed_dir.exists():
        print("âŒ parsed ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. step1.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return []
    
    processed_files = []
    
    # ê° PDFë³„ ë””ë ‰í† ë¦¬ ìˆœíšŒ
    for pdf_dir in parsed_dir.glob("*"):
        if pdf_dir.is_dir():
            pdf_name = pdf_dir.name
            
            # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë“¤ í™•ì¸
            md_with_refs = pdf_dir / f"{pdf_name}-with-image-refs.md"
            md_with_captions = pdf_dir / f"{pdf_name}-with-captions.md"
            images_dir = pdf_dir / f"{pdf_name}-with-image-refs_artifacts"
            
            if md_with_refs.exists() and images_dir.exists():
                # ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ë¡œë“œ
                with open(md_with_refs, 'r', encoding='utf-8') as f:
                    md_content = f.read()
                
                # ì´ë¯¸ì§€ íŒŒì¼ë“¤ í™•ì¸
                image_files = list(images_dir.glob("*.png"))
                
                # ìº¡ì…˜ì´ ì ìš©ëœ ë§ˆí¬ë‹¤ìš´ì´ ìˆìœ¼ë©´ ì‚¬ìš©
                if md_with_captions.exists():
                    with open(md_with_captions, 'r', encoding='utf-8') as f:
                        captioned_content = f.read()
                else:
                    captioned_content = md_content
                
                processed_files.append({
                    'pdf_name': pdf_name,
                    'md_content': md_content,
                    'captioned_content': captioned_content,
                    'image_files': image_files,
                    'images_dir': images_dir
                })
                
                print(f"âœ… {pdf_name} ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
                print(f"   - ë§ˆí¬ë‹¤ìš´: {len(md_content)} ë¬¸ì")
                print(f"   - ì´ë¯¸ì§€: {len(image_files)}ê°œ")
    
    return processed_files

def create_multimodal_sequences(processed_data):
    """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ì¸í„°ë¦¬ë¸Œí•˜ì—¬ ë©€í‹°ëª¨ë‹¬ ì‹œí€€ìŠ¤ ìƒì„±"""
    
    sequences = []
    
    for pdf_data in processed_data:
        pdf_name = pdf_data['pdf_name']
        md_content = pdf_data['md_content']
        captioned_content = pdf_data['captioned_content']
        image_files = pdf_data['image_files']
        images_dir = pdf_data['images_dir']
        
        print(f"\n {pdf_name} ë©€í‹°ëª¨ë‹¬ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")
        
        # ë§ˆí¬ë‹¤ìš´ì—ì„œ ì´ë¯¸ì§€ ì°¸ì¡° ìœ„ì¹˜ ì°¾ê¸°
        image_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        matches = list(re.finditer(image_pattern, md_content))
        
        if matches:
            # ì´ë¯¸ì§€ ì°¸ì¡°ê°€ ìˆëŠ” ê²½ìš°, í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ ì¸í„°ë¦¬ë¸Œ ì‹œí€€ìŠ¤ ìƒì„±
            current_pos = 0
            multimodal_content = []
            
            for i, match in enumerate(matches):
                # ì´ë¯¸ì§€ ì°¸ì¡° ì´ì „ í…ìŠ¤íŠ¸ ì¶”ê°€
                text_before = md_content[current_pos:match.start()].strip()
                if text_before:
                    multimodal_content.append({"type": "text", "content": text_before})
                
                # ì´ë¯¸ì§€ ì¶”ê°€
                if i < len(image_files):
                    image_file = image_files[i]
                    image_name = image_file.name
                    
                    try:
                        # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
                        with open(image_file, "rb") as img_file:
                            img_data = base64.b64encode(img_file.read()).decode('utf-8')
                        
                        # ìº¡ì…˜ëœ ì½˜í…ì¸ ì—ì„œ í•´ë‹¹ ì´ë¯¸ì§€ ë¶€ë¶„ì˜ ìº¡ì…˜ ì°¾ê¸°
                        caption = ""
                        caption_pattern = r'\[ì´ë¯¸ì§€ ìº¡ì…˜: ([^\]]+)\]'
                        caption_matches = re.findall(caption_pattern, captioned_content)
                        if i < len(caption_matches):
                            caption = caption_matches[i]
                        
                        multimodal_content.append({
                            "type": "image", 
                            "content": f"data:image/png;base64,{img_data}",
                            "image_name": image_name,
                            "caption": caption
                        })
                        print(f"   âœ… ì´ë¯¸ì§€ ì¶”ê°€: {image_name}")
                        
                    except Exception as e:
                        print(f"   âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨ {image_name}: {e}")
                
                current_pos = match.end()
            
            # ë§ˆì§€ë§‰ ì´ë¯¸ì§€ ì°¸ì¡° ì´í›„ í…ìŠ¤íŠ¸ ì¶”ê°€
            text_after = md_content[current_pos:].strip()
            if text_after:
                multimodal_content.append({"type": "text", "content": text_after})
            
            # ì‹œí€€ìŠ¤ ìƒì„±
            sequence = {
                'pdf_name': pdf_name,
                'multimodal_content': multimodal_content,
                'image_files': image_files,
                'images_dir': images_dir
            }
            
            sequences.append(sequence)
            print(f"   âœ… ë©€í‹°ëª¨ë‹¬ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: {len(multimodal_content)}ê°œ ìš”ì†Œ")
            
        else:
            # ì´ë¯¸ì§€ ì°¸ì¡°ê°€ ì—†ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
            sequence = {
                'pdf_name': pdf_name,
                'multimodal_content': [{"type": "text", "content": md_content}],
                'image_files': image_files,
                'images_dir': images_dir
            }
            sequences.append(sequence)
            print(f"   âš ï¸ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬: ì´ë¯¸ì§€ ì°¸ì¡° ì—†ìŒ")
    
    return sequences

def create_voyage_multimodal_embeddings(sequences, voyage_client):
    """Voyage AIë¥¼ ì‚¬ìš©í•˜ì—¬ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ìƒì„±"""
    
    print(f"\nğŸ” Voyage AI ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ìƒì„± ì¤‘...")
    
    # ì§€ì›ë˜ëŠ” ëª¨ë¸ í™•ì¸
    try:
        models_response = voyage_client.list_models()
        print(f"   ì§€ì›ë˜ëŠ” ëª¨ë¸: {[model.id for model in models_response.models]}")
    except Exception as e:
        print(f"   ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    embeddings = []
    
    for i, sequence in enumerate(sequences):
        try:
            print(f"   [{i+1}/{len(sequences)}] ì„ë² ë”© ìƒì„±: {sequence['pdf_name']}")
            
            # í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë§Œ ì¶”ì¶œí•˜ì—¬ ì„ë² ë”© (ë©€í‹°ëª¨ë‹¬ API ë¬¸ì œë¡œ ì¸í•´ í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬)
            text_content = []
            for item in sequence['multimodal_content']:
                if item['type'] == 'text':
                    # í•œê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
                    processed_text = preprocess_korean_text(item['content'])
                    if processed_text:
                        text_content.append(processed_text)
                elif item['type'] == 'image':
                    # ì´ë¯¸ì§€ ìº¡ì…˜ì´ ìˆìœ¼ë©´ ì¶”ê°€
                    if item.get('caption'):
                        caption = preprocess_korean_text(item['caption'])
                        text_content.append(f"[ì´ë¯¸ì§€: {caption}]")
                    else:
                        text_content.append("[ì´ë¯¸ì§€]")
            
            # í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
            combined_text = " ".join(text_content)
            
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²­í‚¹
            text_chunks = chunk_text(combined_text, max_length=1500)
            
            print(f"   í…ìŠ¤íŠ¸ ì²­í‚¹: {len(text_chunks)}ê°œ ì²­í¬")
            
            # ê° ì²­í¬ë³„ë¡œ ì„ë² ë”© ìƒì„±
            chunk_embeddings = []
            for j, chunk in enumerate(text_chunks):
                try:
                    # í•œê¸€ ì§€ì›ì´ ë” ì¢‹ì€ ëª¨ë¸ ìš°ì„  ì‚¬ìš©
                    models_to_try = ["voyage-large-2", "voyage-02", "voyage-01"]
                    
                    for model_name in models_to_try:
                        try:
                            result = voyage_client.embed(
                                texts=[chunk],
                                model=model_name
                            )
                            chunk_embeddings.append({
                                'chunk_index': j,
                                'text': chunk,
                                'embedding': result.embeddings[0],
                                'model_used': model_name
                            })
                            print(f"   âœ… ì²­í¬ {j+1} ì„ë² ë”© ì™„ë£Œ (ëª¨ë¸: {model_name})")
                            break
                        except Exception as e:
                            print(f"   âŒ ëª¨ë¸ {model_name} ì‹¤íŒ¨: {e}")
                            continue
                    else:
                        print(f"   âŒ ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"   âŒ ì²­í¬ {j+1} ì„ë² ë”© ì‹¤íŒ¨: {e}")
            
            if chunk_embeddings:
                # ëª¨ë“  ì²­í¬ì˜ ì„ë² ë”©ì„ í‰ê· í•˜ì—¬ í•˜ë‚˜ì˜ ì„ë² ë”©ìœ¼ë¡œ ê²°í•©
                import numpy as np
                avg_embedding = np.mean([chunk['embedding'] for chunk in chunk_embeddings], axis=0)
                
                embeddings.append({
                    'pdf_name': sequence['pdf_name'],
                    'multimodal_content': sequence['multimodal_content'],
                    'embedding': avg_embedding.tolist(),
                    'text': combined_text,
                    'chunks': chunk_embeddings
                })
                
                print(f"   âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ (ì²­í¬ ìˆ˜: {len(chunk_embeddings)})")
            else:
                print(f"   âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: ëª¨ë“  ì²­í¬ ì‹¤íŒ¨")
            
        except Exception as e:
            print(f"   âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
    
    return embeddings

def save_to_qdrant(embeddings, collection_name="voyage-multimodal-docs"):
    """Qdrantì— ì„ë² ë”© ì €ì¥"""
    
    print(f"\n Qdrantì— ì €ì¥ ì¤‘...")
    
    # ë¡œì»¬ Qdrant í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    qdrant_client = QdrantClient("http://localhost:6333")
    
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ
    try:
        qdrant_client.delete_collection(collection_name=collection_name)
        print(f"   ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {collection_name}")
    except:
        pass
    
    # ì²« ë²ˆì§¸ ì„ë² ë”©ì˜ ì°¨ì› í™•ì¸
    if embeddings:
        vector_size = len(embeddings[0]['embedding'])
        print(f"   ë²¡í„° ì°¨ì›: {vector_size}")
    else:
        vector_size = 1536  # voyage-large-2 ê¸°ë³¸ ì°¨ì›
    
    # ì»¬ë ‰ì…˜ ìƒì„±
    qdrant_client.create_collection(
        collection_name=collection_name,
        vectors_config=models.VectorParams(
            size=vector_size,  # ì‹¤ì œ ì„ë² ë”© ì°¨ì› ì‚¬ìš©
            distance=models.Distance.COSINE
        ),
        on_disk_payload=True
    )
    
    print(f"   ì»¬ë ‰ì…˜ ìƒì„±: {collection_name} (ì°¨ì›: {vector_size})")
    
    # í¬ì¸íŠ¸ ìƒì„±
    points = []
    for i, emb_data in enumerate(embeddings):
        # ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸ ì—ì„œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
        text_content = []
        image_info = []
        
        for item in emb_data['multimodal_content']:
            if item['type'] == 'text':
                text_content.append(item['content'])
            elif item['type'] == 'image':
                image_info.append({
                    'image_name': item.get('image_name', ''),
                    'caption': item.get('caption', ''),
                    'image_path': item.get('content', '')[:100] + '...' if len(item.get('content', '')) > 100 else item.get('content', '')
                })
                text_content.append(f"[ì´ë¯¸ì§€: {item.get('caption', '')}]")
        
        combined_text = " ".join(text_content)
        
        point = models.PointStruct(
            id=str(uuid.uuid4()),
            vector=emb_data['embedding'],
            payload={
                'pdf_name': emb_data['pdf_name'],
                'text': combined_text,
                'content_count': len(emb_data['multimodal_content']),
                'image_count': len(image_info),
                'images': image_info
            }
        )
        points.append(point)
    
    # Qdrantì— ì—…ë¡œë“œ
    qdrant_client.upsert(collection_name=collection_name, points=points)
    
    print(f"âœ… {len(points)}ê°œì˜ ì„ë² ë”©ì„ Qdrantì— ì €ì¥ ì™„ë£Œ")
    print(f"   ëŒ€ì‹œë³´ë“œ: http://localhost:6333/dashboard#/collections/{collection_name}")
    
    return qdrant_client, collection_name

def search_similar_documents(qdrant_client, collection_name, query_text, query_image=None, limit=5):
    """ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
    
    print(f"\nğŸ” ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
    
    # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    processed_query = preprocess_korean_text(query_text)
    print(f"   ì¿¼ë¦¬: '{query_text}' -> '{processed_query}'")
    
    # Voyage í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    voyage_client = voyageai.Client()
    
    try:
        # í•œê¸€ ì§€ì›ì´ ë” ì¢‹ì€ ëª¨ë¸ ìš°ì„  ì‚¬ìš©
        models_to_try = ["voyage-large-2", "voyage-02", "voyage-01"]
        query_vector = None
        
        for model_name in models_to_try:
            try:
                result = voyage_client.embed(
                    texts=[processed_query],
                    model=model_name
                )
                query_vector = result.embeddings[0]
                print(f"   ì¿¼ë¦¬ ì„ë² ë”© ì™„ë£Œ (ëª¨ë¸: {model_name})")
                break
            except Exception as e:
                print(f"   ëª¨ë¸ {model_name} ì‹¤íŒ¨: {e}")
                continue
        
        if query_vector is None:
            print(f"   âŒ ëª¨ë“  ëª¨ë¸ì—ì„œ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return []
            
    except Exception as e:
        print(f"   âŒ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return []
    
    # Qdrantì—ì„œ ê²€ìƒ‰
    search_result = qdrant_client.search(
        collection_name=collection_name,
        query_vector=query_vector,
        limit=limit
    )
    
    print(f"ê²€ìƒ‰ ê²°ê³¼:")
    for rank, hit in enumerate(search_result):
        print(f"{rank+1}ìœ„: {hit.payload['pdf_name']}")
        print(f"   ìœ ì‚¬ë„: {hit.score:.4f}")
        print(f"   ì½˜í…ì¸  ìˆ˜: {hit.payload['content_count']}")
        print(f"   ì´ë¯¸ì§€ ìˆ˜: {hit.payload.get('image_count', 0)}")
        
        # í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (í•œê¸€ ê¹¨ì§ ë°©ì§€)
        preview_text = hit.payload['text'][:200]
        if len(hit.payload['text']) > 200:
            preview_text += "..."
        print(f"   í…ìŠ¤íŠ¸: {preview_text}")
        
        # ì´ë¯¸ì§€ ì •ë³´ ì¶œë ¥
        if hit.payload.get('images'):
            print(f"   ì´ë¯¸ì§€ ì •ë³´:")
            for i, img in enumerate(hit.payload['images'][:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                caption_preview = img['caption'][:50] if img['caption'] else "ìº¡ì…˜ ì—†ìŒ"
                print(f"     {i+1}. {img['image_name']} - {caption_preview}...")
            if len(hit.payload['images']) > 3:
                print(f"     ... ì™¸ {len(hit.payload['images']) - 3}ê°œ")
        print()
    
    return search_result

def test_search_functionality(qdrant_client, collection_name):
    """ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    print(f"\nğŸ§ª ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ë‹¤ì–‘í•œ í•œê¸€ ì¿¼ë¦¬ë¡œ í…ŒìŠ¤íŠ¸
    test_queries = [
        "ë”¸ê¸° ë†ì¥ì˜ ì˜¨ë„ ê´€ë¦¬",
        "í™˜ê²½ ì¡°ì ˆ ì‹œìŠ¤í…œ",
        "ë†ì‘ë¬¼ ìƒìœ¡ ê´€ë¦¬",
        "ë¹„ë‹í•˜ìš°ìŠ¤ ì‹œì„¤",
        "ë”¸ê¸° ì¬ë°° ê¸°ìˆ ",
        "ì˜¨ì‹¤ í™˜ê²½ ê´€ë¦¬",
        "ë†ì—… ì‹œì„¤ ìë™í™”",
        "ë”¸ê¸° í’ˆì§ˆ ê´€ë¦¬"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” ì¿¼ë¦¬: '{query}'")
        search_similar_documents(qdrant_client, collection_name, query, limit=3)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    
    # Voyage API í‚¤ í™•ì¸
    if not os.getenv("VOYAGE_API_KEY"):
        print("âŒ VOYAGE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    print("ğŸš€ ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ë° ë²¡í„° ì €ì¥ ì‹œì‘")
    print("=" * 50)
    
    # 1ë‹¨ê³„: step1ì—ì„œ ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
    print("\nï¿½ï¿½ 1ë‹¨ê³„: ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ")
    processed_data = load_processed_data()
    
    if not processed_data:
        print("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. step1.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # 2ë‹¨ê³„: ë©€í‹°ëª¨ë‹¬ ì‹œí€€ìŠ¤ ìƒì„±
    print("\nğŸ”„ 2ë‹¨ê³„: ë©€í‹°ëª¨ë‹¬ ì‹œí€€ìŠ¤ ìƒì„±")
    sequences = create_multimodal_sequences(processed_data)
    
    if not sequences:
        print("âŒ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 3ë‹¨ê³„: Voyage AI ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ìƒì„±
    print("\nï¿½ï¿½ 3ë‹¨ê³„: Voyage AI ì„ë² ë”© ìƒì„±")
    voyage_client = voyageai.Client()
    embeddings = create_voyage_multimodal_embeddings(sequences, voyage_client)
    
    if not embeddings:
        print("âŒ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 4ë‹¨ê³„: Qdrantì— ì €ì¥
    print("\nï¿½ï¿½ 4ë‹¨ê³„: Qdrantì— ì €ì¥")
    qdrant_client, collection_name = save_to_qdrant(embeddings)
    
    # 5ë‹¨ê³„: ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    print("\nğŸ” 5ë‹¨ê³„: ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    test_search_functionality(qdrant_client, collection_name)
    
    print(f"\nâœ… ë‹¨ê³„ 2 ì™„ë£Œ!")
    print(f"   ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´ ë‹¤ìŒ ì‚¬í•­ë“¤ì„ í™•ì¸í•˜ì„¸ìš”:")
    print(f"   - Qdrant ëŒ€ì‹œë³´ë“œ: http://localhost:6333/dashboard")
    print(f"   - ì»¬ë ‰ì…˜: {collection_name}")
    print(f"   - ì €ì¥ëœ ì„ë² ë”© ìˆ˜: {len(embeddings)}ê°œ")

if __name__ == "__main__":
    main()