import logging
import time
from pathlib import Path
from typing import List, TypedDict
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient, models
from qdrant_client.http.models import Distance, VectorParams
from langgraph.graph import StateGraph, END
from langchain_core.documents import Document as LangChainDocument
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import os
from dotenv import load_dotenv

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(level=logging.INFO)
    return logging.getLogger(__name__)

def check_existing_qdrant_collections():
    """ê¸°ì¡´ Qdrant ì»¬ë ‰ì…˜ í™•ì¸"""
    
    print(f"\nğŸ—„ï¸ ê¸°ì¡´ Qdrant ì»¬ë ‰ì…˜ í™•ì¸")
    print("=" * 50)
    
    client = QdrantClient(host="localhost", port=6333)
    
    try:
        collections = client.get_collections()
        print(f"   ë°œê²¬ëœ ì»¬ë ‰ì…˜: {len(collections.collections)}ê°œ")
        
        for collection in collections.collections:
            print(f"   - {collection.name}")
            
            # ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ
            collection_info = client.get_collection(collection_name=collection.name)
            print(f"     ë²¡í„° ìˆ˜: {collection_info.points_count}")
            
            # ë²¡í„° ì„¤ì • ì •ë³´
            if collection_info.config.params.vectors:
                print(f"     ë²¡í„° íƒ€ì…: {type(collection_info.config.params.vectors).__name__}")
        
        return collections.collections
        
    except Exception as e:
        print(f"   âŒ Qdrant ì—°ê²° ì‹¤íŒ¨: {e}")
        return []

def load_existing_qdrant_data(collection_name="voyage-multimodal-docs"):
    """ê¸°ì¡´ Qdrant ë°ì´í„° ë¡œë“œ"""
    
    print(f"\nğŸ“Š ê¸°ì¡´ Qdrant ë°ì´í„° ë¡œë“œ")
    print("=" * 50)
    
    client = QdrantClient(host="localhost", port=6333)
    
    try:
        # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
        collection_info = client.get_collection(collection_name=collection_name)
        print(f"   ì»¬ë ‰ì…˜: {collection_name}")
        print(f"   ì €ì¥ëœ ë²¡í„° ìˆ˜: {collection_info.points_count}")
        
        # ëª¨ë“  í¬ì¸íŠ¸ ì¡°íšŒ
        points = client.scroll(
            collection_name=collection_name,
            limit=1000,  # ìµœëŒ€ 1000ê°œ í¬ì¸íŠ¸ ì¡°íšŒ
            with_payload=True,
            with_vectors=False
        )
        
        print(f"   ë¡œë“œëœ í¬ì¸íŠ¸ ìˆ˜: {len(points[0])}")
        
        # í¬ì¸íŠ¸ ì •ë³´ ì¶œë ¥
        for i, point in enumerate(points[0][:3]):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"   í¬ì¸íŠ¸ {i+1}:")
            print(f"     ID: {point.id}")
            print(f"     PDF: {point.payload.get('pdf_name', 'N/A')}")
            print(f"     ì½˜í…ì¸  ìˆ˜: {point.payload.get('content_count', 'N/A')}")
            print(f"     ì´ë¯¸ì§€ ìˆ˜: {point.payload.get('image_count', 'N/A')}")
            print(f"     í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {point.payload.get('text', '')[:100]}...")
        
        if len(points[0]) > 3:
            print(f"   ... ì™¸ {len(points[0]) - 3}ê°œ í¬ì¸íŠ¸")
        
        return points[0]
        
    except Exception as e:
        print(f"   âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

def create_retriever_from_existing_data(collection_name="voyage-multimodal-docs"):
    """ê¸°ì¡´ Qdrant ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±"""
    
    print(f"\nğŸ” ê¸°ì¡´ ë°ì´í„° ê¸°ë°˜ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±")
    print("=" * 50)
    
    client = QdrantClient(host="localhost", port=6333)
    
    # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (Dense ê²€ìƒ‰)
    dense_embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        api_key=os.getenv("OPENAI_API_KEY")
    )
    
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ì‚¬ìš©í•œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    qdrant = QdrantVectorStore(
        client=client,
        collection_name=collection_name,
        embedding=dense_embeddings,
        vector_name="",  # ê¸°ì¡´ ì»¬ë ‰ì…˜ê³¼ í˜¸í™˜ë˜ë„ë¡ ë¹ˆ ë¬¸ìì—´ ì‚¬ìš©
    )
    
    # ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
    retriever = qdrant.as_retriever(
        search_kwargs={"k": 10}
    )
    
    print(f"   âœ… ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì™„ë£Œ (ìƒìœ„ 10ê°œ ë¬¸ì„œ ê²€ìƒ‰)")
    
    return retriever

def print_search_results(docs, title):
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¹”ë”í•˜ê²Œ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜"""
    
    print(f"\n{title}")
    print("=" * 80)
    print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}ê°œ\n")
    
    for i, doc in enumerate(docs, 1):
        print(f"ğŸ“„ ë¬¸ì„œ {i}")
        print(f"ğŸ“‚ ì¶œì²˜: {doc.metadata.get('pdf_name', 'N/A')}")
        print(f"ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:")
        print(doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content)
        print("-" * 40)

def setup_rag_workflow(retriever):
    """LangGraphë¥¼ ì‚¬ìš©í•œ RAG ì›Œí¬í”Œë¡œìš° ì„¤ì •"""
    
    print(f"\nğŸ¤– RAG ì›Œí¬í”Œë¡œìš° ì„¤ì •")
    print("=" * 50)
    
    # State ì •ì˜
    class RAGState(TypedDict):
        question: str
        documents: List[LangChainDocument]
        answer: str
    
    # LLM ì´ˆê¸°í™” (GPT API ì‚¬ìš©)
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        api_key=os.getenv("OPENAI_API_KEY")
    )
    
    # RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    rag_prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œë“¤:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€:
""")
    
    def retrieve_documents(state: RAGState) -> RAGState:
        """ë¬¸ì„œ ê²€ìƒ‰ ë‹¨ê³„"""
        print("ğŸ“š ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
        question = state["question"]
        documents = retriever.invoke(question)
        print(f"âœ… {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.")
        
        return {
            "question": question,
            "documents": documents,
            "answer": ""
        }
    
    def generate_answer(state: RAGState) -> RAGState:
        """ë‹µë³€ ìƒì„± ë‹¨ê³„"""
        print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
        question = state["question"]
        documents = state["documents"]
        
        # ë¬¸ì„œ ë‚´ìš©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        context = "\n\n".join([doc.page_content for doc in documents])
        
        # LLM ì²´ì¸ êµ¬ì„±
        chain = rag_prompt | llm | StrOutputParser()
        
        # ë‹µë³€ ìƒì„±
        answer = chain.invoke({
            "context": context,
            "question": question
        })
        
        print("âœ… ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return {
            "question": question,
            "documents": documents,
            "answer": answer
        }
    
    # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
    workflow = StateGraph(RAGState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("generate", generate_answer)
    
    # ì—£ì§€ ì¶”ê°€
    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", END)
    
    # ê·¸ë˜í”„ ì»´íŒŒì¼
    rag_app = workflow.compile()
    
    print("   âœ… RAG ì›Œí¬í”Œë¡œìš° ì„¤ì • ì™„ë£Œ")
    
    return rag_app

def interactive_qa(rag_app):
    """ëŒ€í™”í˜• Q&A ì‹œìŠ¤í…œ"""
    
    print(f"\nğŸ’¬ ëŒ€í™”í˜• Q&A ì‹œìŠ¤í…œ")
    print("=" * 50)
    print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”. 'quit', 'exit', ë˜ëŠ” 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    print("ë¬¸ì„œì— ëŒ€í•œ ì§ˆë¬¸ì„ ììœ ë¡­ê²Œ í•´ì£¼ì„¸ìš”!")
    print("-" * 50)
    
    while True:
        try:
            question = input("\nâ“ ì§ˆë¬¸: ").strip()
            
            if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ ëŒ€í™”í˜• Q&A ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not question:
                continue
            
            print("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘...")
            
            initial_state = {"question": question, "documents": [], "answer": ""}
            result = rag_app.invoke(initial_state)
            
            print(f"\nğŸ¤– ë‹µë³€:")
            print(result["answer"])
            print(f"\nğŸ“š ì°¸ì¡°ëœ ë¬¸ì„œ ìˆ˜: {len(result['documents'])}ê°œ")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ëŒ€í™”í˜• Q&A ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    
    print("ğŸš€ RAG ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 50)
    
    # 1ë‹¨ê³„: ê¸°ì¡´ Qdrant ì»¬ë ‰ì…˜ í™•ì¸
    print("\nğŸ—„ï¸ ê¸°ì¡´ Qdrant ì»¬ë ‰ì…˜ í™•ì¸ ì¤‘...")
    collections = check_existing_qdrant_collections()
    
    if not collections:
        print("âŒ Qdrantì— ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. step1.pyì™€ step2.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    # 2ë‹¨ê³„: ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“Š ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì¤‘...")
    existing_data = load_existing_qdrant_data()
    
    if not existing_data:
        print("âŒ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 3ë‹¨ê³„: ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
    print("\nğŸ” ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± ì¤‘...")
    retriever = create_retriever_from_existing_data()
    
    # 4ë‹¨ê³„: RAG ì›Œí¬í”Œë¡œìš° ì„¤ì •
    print("\nğŸ¤– RAG ì›Œí¬í”Œë¡œìš° ì„¤ì • ì¤‘...")
    rag_app = setup_rag_workflow(retriever)
    
    # 5ë‹¨ê³„: ëŒ€í™”í˜• Q&A ì‹œìŠ¤í…œ ì‹œì‘
    print("\nğŸ’¬ ëŒ€í™”í˜• Q&A ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    interactive_qa(rag_app)
    
    print(f"\nâœ… RAG ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"   - Qdrant ëŒ€ì‹œë³´ë“œ: http://localhost:6333/dashboard")
    print(f"   - ê¸°ì¡´ ë°ì´í„° í™œìš©: {len(existing_data)}ê°œ í¬ì¸íŠ¸")
    print(f"   - Dense ë²¡í„° ê²€ìƒ‰ ì§€ì› (OpenAI)")
    print(f"   - ëŒ€í™”í˜• Q&A ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")

if __name__ == "__main__":
    main() 